{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6hoLmML4IUUBDlqtbf0+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanalpillai/Titanic-Machine-Learning-from-Disaster/blob/main/Titanic_Machine_Learning_from_Disaster_%7C_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries and Loading Data**"
      ],
      "metadata": {
        "id": "XDO8-00UdAu8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EAFpRsSBa9RE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "UdaspmGecrzp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic overview\n",
        "print(train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMYEPM2ycuy3",
        "outputId": "1c0c9fb9-890d-4d13-a0ee-97c6ee310c48"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we begin by importing essential Python libraries for data manipulation and machine learning. Then, we load the Titanic dataset (both the training and test sets) which we'll use throughout our analysis and model building. The datasets are assumed to be stored locally and named train.csv for the training data and test.csv for the test set. This step is crucial for getting our environment set up and data ready for preprocessing and analysis."
      ],
      "metadata": {
        "id": "MtMcaLWPcnLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KM1TDqpwdF2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(df):\n",
        "    # Extract titles from names\n",
        "    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "\n",
        "    # Simplify titles\n",
        "    df['Title'] = df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr',\n",
        "                                       'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "\n",
        "    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
        "    df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
        "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
        "\n",
        "    # Creating FamilySize\n",
        "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
        "\n",
        "    # IsAlone feature\n",
        "    df['IsAlone'] = 0\n",
        "    df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering to both train and test sets\n",
        "train = feature_engineering(train)\n",
        "test = feature_engineering(test)"
      ],
      "metadata": {
        "id": "lRhI-96Ecy_d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we focus on creating new features that could help improve the model's predictive performance. Feature engineering is a critical process where domain knowledge is applied to create features that make machine learning algorithms work. We extract titles from passenger names, which can provide insights into social status, gender, and marital status. Additionally, we create a FamilySize feature by adding SibSp and Parch, plus one for the passenger themselves. This helps us capture the size of a passenger's family aboard. These engineered features can significantly influence the model's predictions."
      ],
      "metadata": {
        "id": "bfYmu_asc1uG"
      }
    }
  ]
}
