{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHE1Au2HMZS34dlH13ImcB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanalpillai/Titanic-Machine-Learning-from-Disaster/blob/main/Titanic_Machine_Learning_from_Disaster_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries and Loading Data**"
      ],
      "metadata": {
        "id": "XDO8-00UdAu8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EAFpRsSBa9RE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "UdaspmGecrzp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic overview\n",
        "print(train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMYEPM2ycuy3",
        "outputId": "1c0c9fb9-890d-4d13-a0ee-97c6ee310c48"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we begin by importing essential Python libraries for data manipulation and machine learning. Then, we load the Titanic dataset (both the training and test sets) which we'll use throughout our analysis and model building. The datasets are assumed to be stored locally and named train.csv for the training data and test.csv for the test set. This step is crucial for getting our environment set up and data ready for preprocessing and analysis."
      ],
      "metadata": {
        "id": "MtMcaLWPcnLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering**"
      ],
      "metadata": {
        "id": "KM1TDqpwdF2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(df):\n",
        "    # Extract titles from names\n",
        "    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "\n",
        "    # Simplify titles\n",
        "    df['Title'] = df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr',\n",
        "                                       'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "\n",
        "    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
        "    df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
        "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
        "\n",
        "    # Creating FamilySize\n",
        "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
        "\n",
        "    # IsAlone feature\n",
        "    df['IsAlone'] = 0\n",
        "    df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering to both train and test sets\n",
        "train = feature_engineering(train)\n",
        "test = feature_engineering(test)"
      ],
      "metadata": {
        "id": "lRhI-96Ecy_d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we focus on creating new features that could help improve the model's predictive performance. Feature engineering is a critical process where domain knowledge is applied to create features that make machine learning algorithms work. We extract titles from passenger names, which can provide insights into social status, gender, and marital status. Additionally, we create a FamilySize feature by adding SibSp and Parch, plus one for the passenger themselves. This helps us capture the size of a passenger's family aboard. These engineered features can significantly influence the model's predictions."
      ],
      "metadata": {
        "id": "bfYmu_asc1uG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing and Model Pipeline**"
      ],
      "metadata": {
        "id": "QImNdBuod7Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting features for the model\n",
        "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"Title\", \"FamilySize\", \"IsAlone\"]\n",
        "X = train[features]\n",
        "y = train[\"Survived\"]\n",
        "X_test = test[features]\n",
        "\n",
        "# Preprocessing for numerical and categorical data\n",
        "numerical_cols = [\"SibSp\", \"Parch\", \"FamilySize\"]\n",
        "categorical_cols = [\"Pclass\", \"Sex\", \"Embarked\", \"Title\", \"IsAlone\"]\n",
        "\n",
        "# Preprocessing pipeline\n",
        "numerical_transformer = SimpleImputer(strategy=\"constant\")\n",
        "\n",
        "# One-hot encode categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numerical_transformer, numerical_cols),\n",
        "    ('cat', categorical_transformer, categorical_cols)\n",
        "])\n",
        "\n",
        "# Model pipeline\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                        ('model', RandomForestClassifier(n_estimators=100, random_state=0))])"
      ],
      "metadata": {
        "id": "VREVgzXGd0ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll prepare our data for modeling. This involves handling missing values and encoding categorical variables. We use SimpleImputer to fill in missing values and OneHotEncoder to convert categorical variables into a format that can be provided to machine learning algorithms. We then construct a preprocessing pipeline that applies these transformations to the appropriate columns. This pipeline ensures that our data preprocessing steps are organized and can be easily applied to both training and testing data."
      ],
      "metadata": {
        "id": "C1rY_z7cd4A0"
      }
    }
  ]
}